{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Hands-on\n",
    "\n",
    "**First day**     \n",
    "\n",
    "At the end of this day we \n",
    "- Explore our dataset\n",
    "- Make some plots to check different variables\n",
    "- Select interesting features to apply ML algorithms\n",
    "- Fill missing values\n",
    "- Transform some feature to more infomative variables\n",
    "\n",
    "[Pandas cheatsheet](https://github.com/creyesp/houses-project/blob/add-binder-configs/pandas_cheatsheet.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are some questions that I can answer with this dataset?\n",
    "Understand your dataset is the fist step of any datascience project. You need to know the limitations and make a list of posible question that could be answering with this dataset. These question can reduce, expande or modify the scope of our project.\n",
    "\n",
    "examples: \n",
    "- We could have great ideas but poor data\n",
    "- We could have incorect question for our dataset\n",
    "- We could have a rich dataset ....\n",
    "\n",
    "**Data**: \n",
    "- We have a set of features of houses for sale in a specific time windows.  \n",
    "\n",
    "**Business question/objective**:\n",
    "- **New infocasas functionality**: Is it possible to offer an estimated price for selling given house characteristics (uploaded by owner in the webpage) without asking an appraiser? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exploratory data analysis\n",
    "- How many rows are in our dataset?\n",
    "- How many columns are in our dataset?\n",
    "- What data types are in the columns?\n",
    "- are there missing values in the dataset? Do we infer missing values? how?\n",
    "- are there outlier values? \n",
    "\n",
    "Data types:\n",
    "- **Numeric**:\n",
    "    - *Discrete*: variables that have finite posible values.\n",
    "    - *Continuous*:  variables that can have an infinite number of possible values\n",
    "- **Categorical, variables that have 2 or more possible values**:\n",
    "    - *Ordinal*: these values have a meaningful order or rank. Ex. marks, A, B, C\n",
    "    - *Nominal*: the order of those values have no meaning. Ex, names\n",
    "    - *Binary or Dichotomous*: only 2 posible values, 1/0\n",
    "- **Unstructured**:\n",
    "    - *text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom module\n",
    "import handson\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_columns = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "path_file = '../data/preprocessed/dataset.csv'\n",
    "\n",
    "# Read csv file and assign to df variable\n",
    "df = pd.read_csv(path_file)\n",
    "df = df.loc[df['precio']< 1.5e6, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information about our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the neme of columns (features)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 row of our dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data types of columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are missing values in each columns\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of missing values from each columns\n",
    "df.notnull().sum() / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of unique values for each feature\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic resume\n",
    "### Numeric variables\n",
    "Look at statistic info for each columns and check which columns has unusual behavior. \n",
    "- Are all positive values?\n",
    "- is standard deviation different to zero?\n",
    "- How long is percetil 75 from max?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a resume of numerical columns from our dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.quantile([0.01, 0.95]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categeries resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a resume of no numerical columns from our dataset. \n",
    "# Hint: use include='O' as argument in resume function\n",
    "\n",
    "df.describe(include='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization\n",
    "[Seaborn](https://seaborn.pydata.org/) is a very useful package to make EDA (built on [Matplotlib](https://matplotlib.org/)), it's a statistical data visualization package and it's easy to create univarible and bivarible plots.\n",
    "<img src=\"img/seaborn.png\" />\n",
    "\n",
    "### Univarible plots\n",
    "- Distribution\n",
    "- Histograms\n",
    "- Boxplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot price distribution\n",
    "f, ax = plt.subplots()\n",
    "sns.distplot(df['precio'].dropna(), ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot boxplots of price grop by some categorical feature \n",
    "# ex. estado, barrio, banos, dormitorios, tipo_propiedad\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.boxplot(x='banos', y='precio', data=df)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.boxplot(x='dormitorios', y='precio', data=df)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.boxplot(y='barrio', x='precio', orient='h', data=df)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.boxplot(x='estado', y='precio', data=df, ax=ax)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.boxplot(y='tipo_propiedad', x='precio', orient='h', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of barrio feature\n",
    "f, ax = plt.subplots()\n",
    "df['barrio'].value_counts().plot(kind='bar', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram of \"ano_de_construccion\" between 1880 and 2019, in bins of 10 years\n",
    "df['ano_de_construccion'].plot.hist(bins=np.arange(1880, 2020, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bivarible plots\n",
    "- Scatter\n",
    "- Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "sns.scatterplot(x='ano_de_construccion', y='precio', data=df, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gastos_comunes'].nlargest(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "sns.scatterplot(x='gastos_comunes', y='precio', data=df, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    x=\"m2_edificados\", \n",
    "    y=\"precio\",\n",
    "    col=\"banos\",\n",
    "    col_wrap=2,\n",
    "    hue='dormitorios',\n",
    "    data=df,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'banos',\n",
    "    'dormitorios',\n",
    "    'garajes',\n",
    "    'gastos_comunes',\n",
    "    'm2_de_la_terraza',\n",
    "    'm2_del_terreno',\n",
    "    'm2_edificados',\n",
    "    'plantas',\n",
    "    'precio',\n",
    "]\n",
    "sns.pairplot(df[features]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'ano_de_construccion',\n",
    "    'banos',\n",
    "    'dormitorios',\n",
    "    'gastos_comunes',\n",
    "    'm2_de_la_terraza',\n",
    "    'm2_del_terreno',\n",
    "    'm2_edificados',\n",
    "    'precio',\n",
    "]\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(df[features].corr(), annot=True, fmt='.02f', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepratation\n",
    "\n",
    "**\"The quality and quantity of data that you gather will directly determine how good your predictive model can be.\"**\n",
    "\n",
    "- Select relevant features\n",
    "- Clean and Missing values imputetion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Feature selection</th>\n",
    "    <th>Filling missing values</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"http://dkopczyk.quantee.co.uk/wp-content/uploads/2018/10/feat_sel-600x265.png\" /></th>\n",
    "    <th><img src=\"https://i.stack.imgur.com/E4mhD.png\" /></th>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select feature for analysis\n",
    "Check dataset [documentation](https://github.com/creyesp/houses-project/blob/add-binder-configs/data/dataset_description.md) to choice the most interesting feature to answer our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_analyze = [\n",
    "    'ano_de_construccion', \n",
    "    'banos',\n",
    "    'banos_extra',\n",
    "    'descripcion',\n",
    "    'disposicion',\n",
    "    'distancia_al_mar',\n",
    "    'dormitorios',\n",
    "    'dormitorios_extra',\n",
    "    'estado',\n",
    "    'extra',\n",
    "    'garajes',\n",
    "    'garajes_extra',\n",
    "    'gastos_comunes',\n",
    "    'tipo_de_publicacion',\n",
    "    'm2_de_la_terraza',\n",
    "    'm2_del_terreno',\n",
    "    'm2_edificados',\n",
    "    'oficina',\n",
    "    'penthouse',\n",
    "    'plantas',\n",
    "    'plantas_extra',\n",
    "    'precio',\n",
    "    'sobre',\n",
    "    'tipo_propiedad',\n",
    "    'vista_al_mar',\n",
    "    'vivienda_social',\n",
    "    'barrio', \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset in numerical and string variables\n",
    "Pandas has a method to split dataset group by dtypes:\n",
    "- **'object'**: To select strings you must use the object dtype\n",
    "- **'number'**: To select all numeric types\n",
    "- **'category'**: To select Pandas categorical dtypes\n",
    "- **'datetime'**: To select datetimes\n",
    "- **'timedelta'**: To select timedeltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df[columns_to_analyze].select_dtypes(include='number')\n",
    "df_obj = df[columns_to_analyze].select_dtypes(include='object')\n",
    "\n",
    "print('Numerical columns: {}\\n'.format(df_num.columns.tolist()))\n",
    "print('Caterorial columns: {}'.format(df_obj.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values imputation\n",
    "Some features have only 1 valid value and the rest of the values are Nan (Not a number), ex. \"oficina\" column. In this case we can infer that missing value is the opossite value. \n",
    "- **Look at what feature we can replace Nan values with 0**.\n",
    "\n",
    "There are other features that nan values should be replacing with a especifi value, ex. \"plantas\", if a house or apartment hasn't a valid value then default value should be 1.\n",
    "- **Look at what feature we can replace Nan values with especific values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with zero\n",
    "fill_zero_col = [\n",
    "    'm2_de_la_terraza',\n",
    "    'vivienda_social',\n",
    "    'gastos_comunes',\n",
    "    'garajes',\n",
    "    'garajes_extra',\n",
    "    'plantas_extra',\n",
    "    'penthouse',\n",
    "    'oficina',\n",
    "    'vista_al_mar',\n",
    "]\n",
    "df_num.loc[:, fill_zero_col] = df_num[fill_zero_col].fillna(0)\n",
    "\n",
    "# Fill missing values with 1\n",
    "df_num['plantas'].fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can infer some values of a column from other column, for example we can fill nan values in \"m2_del_terreno\" from \"m2_edificados\".\n",
    "- **Select nan values from  \"m2_del_terreno\" and fill it with \"m2_edificados\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing value usings other columns\n",
    "mask_m2_terreno = df_num['m2_del_terreno'].isna()\n",
    "df_num.loc[mask_m2_terreno, 'm2_del_terreno'] = df_num.loc[mask_m2_terreno, 'm2_edificados']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can use some statistic metrics to fill missing values, like mean, median, mode, etc.\n",
    "- **Compute the median of \"m2_edificados\" and fill nan values with this result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num['m2_edificados'].fillna(df_num['m2_edificados'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical feature we can add a new category to fill missing values\n",
    "- **Replace nan values with a defaul category for following feature:**\n",
    "    - \"barrio\"\n",
    "    - \"disposicion\"\n",
    "    - \"tipo_propiedad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categories\n",
    "df_obj['barrio'].fillna('desconocido', inplace=True)\n",
    "df_obj['disposicion'].fillna('otro', inplace=True)\n",
    "df_obj['tipo_propiedad'].fillna('otros', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature transformation\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create new features applying some functions or filters to transform them and get a more informative features. Apply the following transformation:\n",
    "- **Create a binary feature called \"cerca_rambla\" which is 1 when \"distancia_al_mar\" < 1000 or \"vista_al_mar\" is 1, in other case set it to 0.**\n",
    "- **Create a feature called \"m2_index\" which is the ratio between \"m2_edificados\" and \"m2_del_terreno\"**\n",
    "- **Create a binary feature called \"es_casa\" which is 1 if \"tipo_propiedad\" == 'casas' and 0 is \"tipo_propiedad\" == 'apartamentos'.**\n",
    "- **Create a binary feature called \"parrilero\" if \"extra\" feature contain 'parrillero'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num['cerca_rambla'] = (df_num['distancia_al_mar'] <= 1000) | (df_num['vista_al_mar'] ).astype(float)\n",
    "\n",
    "df_num['m2_index'] = df_num['m2_edificados']/df_num['m2_del_terreno']\n",
    "\n",
    "df_num['es_casa'] = df_obj['tipo_propiedad'].map({'casas':1, 'apartamentos': 0})\n",
    "\n",
    "df_num['parrilero'] = df_obj['extra'].str.contains('parrillero').fillna(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning\n",
    "Some variable like years or ages is an example of a feature type that might benefit from transformation into a discrete variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_decade = np.arange(1880, 2021, 10)\n",
    "range_label = np.arange(1880, 2020, 10)\n",
    "year = df['ano_de_construccion'].copy()\n",
    "year[year < 1880] = 1880\n",
    "year[year > 2019] = 2019\n",
    "year.fillna(1951, inplace=True)\n",
    "df_num['decada'] = pd.cut(year,\n",
    "                    bins=range_decade,\n",
    "                    labels=range_label, \n",
    "                    right=False\n",
    "                   ).astype(int)\n",
    "df_num['decada']\n",
    "f, ax = plt.subplots()\n",
    "df_num['decada'].plot.hist(bins=np.arange(1870, 2020, 10), ax=ax)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "sns.boxenplot(x='decada', y='precio', data=df_num, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some categorical features are ordinal, then we can map them to a numerical values in a especific order\n",
    "- **Create a dictionary with all posible values of \"estado\" feature and assigne a numerical value, where min value is the worse status and the max value is the best status of properties. Then map these values to a \"estado\" feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical transformation\n",
    "map_status = {\n",
    "    'en construccion': 3,\n",
    "    'a estrenar': 3,\n",
    "    'excelente estado': 3,\n",
    "    'buen estado': 2,\n",
    "    'reciclada': 2,\n",
    "    'requiere mantenimiento': 1,\n",
    "    'a reciclar': 0,\n",
    "#     '': np.nan,\n",
    "}\n",
    "df_num['estado'] = df_obj['estado'].map(map_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful transformation is [80/20 rule or Pareto Rule](https://en.wikipedia.org/wiki/Pareto_principle), it's say that  for many events, roughly 80% of the effects come from 20% of the causes. In our case \"barrio\" feature has a similar behaviour.  \n",
    "<img src=\"https://www.dansilvestre.com/wp-content/uploads/2017/12/DanSilvestre.com_-1.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,5))\n",
    "(df_obj['barrio'].value_counts().cumsum()/df_obj['barrio'].count()).plot(kind='bar', ax=ax)\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize('x-small') \n",
    "ax.grid(axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominal features like \"barrio\" can be transformed to a numerical varable applying **ONE-HOT encoding**.\n",
    "<img src=\"img/one-hot-encoding.png\" />\n",
    "\n",
    "- **Apply one-hot encoding on Pareto's transformation of \"bario\" feature and add prefix='ZN_', then assign to zona variable.**\n",
    "- **Apply one-hot encoding on \"disposicion\" feature and add prefix='DISP_', then assign to zona disp.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zona = pd.get_dummies(handson.pareto_rule(df_obj['barrio']), prefix='ZN_',)\n",
    "disp = pd.get_dummies(df_obj['disposicion'], prefix='DISP_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally concatenate all new features and drop redundant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_final = pd.concat([df_num, zona, disp], axis=1)\n",
    "drop_col = ['distancia_al_mar', 'vista_al_mar', 'm2_del_terreno', 'ano_de_construccion']\n",
    "df_num_final.drop(columns=drop_col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply customs filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_final.quantile([0.05, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (df_num_final.fillna(0) >= 0).all(axis=1)\n",
    "    & df_obj['tipo_propiedad'].isin(['apartamentos', 'casas'])\n",
    "    & (df_num_final['decada'].between(1880, 2019))\n",
    "    & (df_num_final['oficina'] != 1)\n",
    "    & (df_num_final['penthouse'] != 1) \n",
    "    & (df_num_final['banos'].between(1, 3))\n",
    "    & (df_num_final['dormitorios'].between(0, 5))\n",
    "    & (df_num_final['garajes'].between(0, 5))\n",
    "    & (df_num_final['m2_de_la_terraza'].between(0, 400))\n",
    "    & (df_num_final['m2_edificados'] >= 10)\n",
    "    & (df_num_final['gastos_comunes'].between(0, 5e4))\n",
    "    & (df_num_final['precio'].between(1e4, 2e6))\n",
    "    & (df_num_final['m2_index'].between(0, 4))\n",
    ")\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop no informative columns and drop missing row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_std_col = df_num_final.columns[df_num_final[mask].std() == 0]\n",
    "df_final = df_num_final[mask].drop(columns=zero_std_col).astype(float).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handson.info(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save ready dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('../data/ready/dataset_houses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "The continuous variables in our dataset are at varying scales.\n",
    "This poses a problem for many popular machine learning algorithms which often use Euclidian distance between data points to make the final predictions. Standardising the scale for all continuous variables can often result in an increase in performance of machine learning models.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = QuantileTransformer(output_distribution='normal') \n",
    "# scaler = PowerTransformer(method='yeo-johnson')\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Feature Selection?\n",
    "- Overfitting\n",
    "- Occam’s Razor: \"Entities should not be multiplied without necessity.\"  William of Ockham\n",
    "- Garbage In Garbage out: Most of the times, we will have many non-informative features.\n",
    "\n",
    "Type of Feature Selection:\n",
    "\n",
    "- Filter based: We specify some metric and based on that filter features. An example of such a metric could be correlation/chi-square.\n",
    "- Wrapper-based: Wrapper methods consider the selection of a set of features as a search problem. Example: Recursive Feature Elimination\n",
    "- Embedded: Embedded methods use algorithms that have built-in feature selection methods. For instance, Lasso and RF have their own feature selection methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = 20\n",
    "feature_name = data_set.columns.to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter based\n",
    "Pearson correlation\n",
    "We check the absolute value of the Pearson’s correlation between the target and numerical features in our dataset. We keep the top n features based on this criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_final.corr()['precio'].drop('precio')\n",
    "high_corr = np.abs(corr) > 0.5\n",
    "corr_feature = high_corr.index.tolist()\n",
    "corr_support = high_corr.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "rfe_selector = RFE(estimator=LinearRegression(), n_features_to_select=num_feats, step=1, verbose=5, )\n",
    "rfe_selector.fit(data_set, target)\n",
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = data_set.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embebed\n",
    "Embedded methods use algorithms that have built-in feature selection methods.\n",
    "\n",
    "### Lasso\n",
    "For example, Lasso and RF have their own feature selection methods. Lasso Regularizer forces a lot of feature weights to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "embeded_lr_selector = SelectFromModel(Lasso(), max_features=num_feats)\n",
    "embeded_lr_selector.fit(data_set, target)\n",
    "\n",
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = data_set.loc[:,embeded_lr_support].columns.tolist()\n",
    "print(str(len(embeded_lr_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lr_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest\n",
    "We can also use RandomForest to select features based on feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "embeded_rf_selector = SelectFromModel(RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=10), max_features=num_feats)\n",
    "embeded_rf_selector.fit(data_set, target)\n",
    "\n",
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = data_set.loc[:,embeded_rf_support].columns.tolist()\n",
    "print(str(len(embeded_rf_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "embeded_rf_selector = SelectFromModel(DecisionTreeRegressor(), max_features=num_feats)\n",
    "embeded_rf_selector.fit(data_set, target)\n",
    "\n",
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = data_set.loc[:,embeded_rf_support].columns.tolist()\n",
    "print(str(len(embeded_rf_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectFromModel\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbr=LGBMRegressor(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "\n",
    "embeded_lgb_selector = SelectFromModel(lgbr, max_features=num_feats)\n",
    "embeded_lgb_selector.fit(data_set, target)\n",
    "\n",
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = data_set.loc[:,embeded_lgb_support].columns.tolist()\n",
    "print(str(len(embeded_lgb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all selection together\n",
    "feature_selection_df = pd.DataFrame(\n",
    "    {\n",
    "        'Feature':feature_name, \n",
    "        'Pearson':corr_support, \n",
    "        'RFE':rfe_support, \n",
    "        'Logistics':embeded_lr_support,\n",
    "        'Random Forest':embeded_rf_support,\n",
    "        'LightGBM':embeded_lgb_support\n",
    "    })\n",
    "# count the selected times for each feature\n",
    "feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# display the top 100\n",
    "feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "feature_selection_df.head(num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pointbiserialr(df_final['precio'], df_final['banos_extra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pointbiserialr(df_final['precio'], df_final['banos_extra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(df_final['dormitorios'], df_final['banos'])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramer = cramers_corrected_stat(confusion_matrix)\n",
    "cramer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
