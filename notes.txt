Data preparation, cleaning, pre-processing, cleansing, wrangling.

Wikipedia defines data cleansing as:

    ...is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting. 
    procesos estadisticos, visualizacion, e

KDD Process (left) and the CRISP-DM model (right).
https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining#/media/File:CRISP-DM_Process_Diagram.png

Data preparation can be seen in the CRISP-DM model (though it can be reasonably argued that "data understanding" falls within our definition as well). We can also equate our data preparation with the framework of the KDD Process — specifically the first 3 major steps — which are selection, preprocessing, and transformation. 

python ecosystem 
pandas | seaborn | matplotlib | 

Step 1: Preparing for the Preparation
Step 2: Exploratory Data Analysis 
	Exploratory data Analysis (EDA) is an integral aspect of any greater data analysis, data science, or machine learning project. Understanding data before working with it isn't just a pretty good idea, it is a priority if you plan on accomplishing anything of consequence. 

	-- The purpose of EDA is to use summary statistics and visualizations to better understand data, and find clues about the tendencies of the data, its quality and to formulate assumptions and the hypothesis of our analysis.  Also validate your dataset 

	 Structured data: 
		catagotical data (discrete): frequency table or describe
			- norminal = values represent descrete units/ changing the order of units doesn't change their value
			- ordinal = values represent descrete units and ordered units/ Distance between units is not the same
		Numerical Values (Continuos data): statustic measure
			- Interval
			- ratio = The ratio scale of measurement is the most informative scale.
		Binning -> numeric to categorical
	Unstructurated data:
		- photos, images, audio, language text and many others. 
Step 3: Missing Values
	use knowdlege about problem to fill missing values
	Some commonly used methods for dealing with missing values include:

	    dropping instances with missing values
	    dropping attributes with missing values
	    imputing the attribute { mean | median | mode } for all missing values
	    imputing the attribute missing values via linear regression 
Step 4: Outliers
remove or not

Step 5: Imbalanced Data
https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html

Step 6: Data Transformations
Standardization, or mean removal and variance scaling
	- standardization: refers to scaling the data to have a mean of 0 and a standard deviation of 1
	- normalization: refers to the scaling the data values to fit into a predetermined range, generally between 0 and 1
https://scikit-learn.org/stable/modules/preprocessing.html
-StandardScaler (usually called Z-score Standardization)
-MinMaxScaler (usually called min-max Normalization)

Log or exp tranformation to convert a non-lineal variable to a linear variable


acquiring data -> feature engineering-> EDA	->